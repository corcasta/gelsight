{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09764ba4-7205-4e1c-bf6c-8eb1ea500625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50ad7a7f-39b5-4ea5-9319-6600c54be48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48c8f325-4069-49d7-a184-db9283e575f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_dataset(dataset, val_split=0.25):\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['valid'] = Subset(dataset, val_idx)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae910bdb-bb8e-48c3-8e9a-d80d97855a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, loss_criterion, optimizer, epochs, train_data_loader, valid_data_loader, device):\n",
    "    '''\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "  \n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    '''\n",
    "    \n",
    "    writer = SummaryWriter()\n",
    "    model = model.to(device)\n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs.cuda())\n",
    "\n",
    "            #print(f\"Output shape: {outputs.shape} \\t Label shape: {labels.shape}\")\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_criterion(outputs, labels) #.to(torch.float32)\n",
    "            \n",
    "            #print(f\"Outputs: {outputs} \\t type: {outputs.dtype}\")\n",
    "            #print(f\"Loss: {loss} \\t type: {loss.dtype}\")\n",
    "            \n",
    "            # Backpropagate the gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            \n",
    "\n",
    "            # Compute the accuracy\n",
    "            #ret, predictions = torch.max(outputs.data, 1)\n",
    "            #correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            #acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            #train_acc += acc.item() * inputs.size(0)\n",
    "            \n",
    "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "        writer.add_scalar(\"Train_loss x epoch\", train_loss/len(train_data_loader), epoch)\n",
    "        \n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Set to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model\n",
    "                outputs = model(inputs.cuda())\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "\n",
    "                # Compute the total loss for the batch and add it to valid_loss\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                #ret, predictions = torch.max(outputs.data, 1)\n",
    "                #correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                #acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                #valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "            \n",
    "        writer.add_scalar(\"Valid_loss x epoch\", valid_loss/len(valid_data_loader), epoch)\n",
    "        \n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/len(train_data_loader) \n",
    "        #avg_train_acc = train_acc/train_data_size\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/len(valid_data_loader) \n",
    "        #avg_valid_acc = valid_acc/valid_data_size\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss])#, avg_train_acc, avg_valid_acc])\n",
    "                \n",
    "        epoch_end = time.time()\n",
    "    \n",
    "        #print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch+1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, \\n\\t\\tValidation : Loss : {:.4f}, Time: {:.4f}s\".format(epoch+1, avg_train_loss, avg_valid_loss, epoch_end-epoch_start))\n",
    "\n",
    "        # Save if the model has best accuracy till now\n",
    "        #torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n",
    "    writer.close()      \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6570953c-0a43-4cbf-9ede-89f5699a6266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in trainloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            epoch_loss = running_loss / dataset_sizes\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "            # deep copy the model\n",
    "        print()\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d99d88-b902-48ef-8964-bc99c368465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom datasets for each shape\n",
    "class SquareImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = \"square_img_\" + str(self.img_labels.iloc[idx, 0]) + \".jpg\"\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = read_image(img_path)#.float()\n",
    "        label = self.img_labels.iloc[idx, 1:4].to_numpy()\n",
    "        label = np.sqrt((label*label).sum())\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "class SphereImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = \"sphere_img_\" + str(self.img_labels.iloc[idx, 0]) + \".jpg\"\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = read_image(img_path)#.float()\n",
    "        label = self.img_labels.iloc[idx, 1:4].to_numpy()\n",
    "        label = np.sqrt((label*label).sum())\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "class RomboidImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = \"romboid_img_\" + str(self.img_labels.iloc[idx, 0]) + \".jpg\"\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = read_image(img_path)#.float()\n",
    "        label = self.img_labels.iloc[idx, 1:4].to_numpy()\n",
    "        label = np.sqrt((label*label).sum())\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ca06ae-9684-433e-9e84-27c4dc828c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: imgs and labels paths\n",
    "dataset_path = \"/home/corcasta/Documents/ati/dataset\"\n",
    "square_imgs_dir = dataset_path + \"/images/square\"\n",
    "sphere_imgs_dir = dataset_path + \"/images/sphere\"\n",
    "romboid_imgs_dir = dataset_path + \"/images/romboid\"\n",
    "\n",
    "square_labels_dir = dataset_path + \"/labels/square\"\n",
    "sphere_labels_dir = dataset_path + \"/labels/sphere\"\n",
    "romboid_labels_dir = dataset_path + \"/labels/romboid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5366fe0a-933f-48e0-b464-7db2e8e49e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_df = pd.read_csv(square_labels_dir + \"/square_data.csv\")\n",
    "sphere_df = pd.read_csv(sphere_labels_dir + \"/sphere_data.csv\")\n",
    "romboid_df = pd.read_csv(romboid_labels_dir + \"/romboid_data.csv\")\n",
    "total_df = pd.concat([square_df, sphere_df], ignore_index=True)\n",
    "\n",
    "# Getting all the labels in ORDER\n",
    "labels = total_df[[\"fx\", \"fy\", \"fz\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb761964-4a4d-4ea6-9c30-eda2f8717b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fx</th>\n",
       "      <th>fy</th>\n",
       "      <th>fz</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>x_shear</th>\n",
       "      <th>y_shear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.000228</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.000644</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.019416</td>\n",
       "      <td>0.034281</td>\n",
       "      <td>-0.470873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.020019</td>\n",
       "      <td>0.034907</td>\n",
       "      <td>-0.463959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>0.035586</td>\n",
       "      <td>-0.460220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        fx        fy        fz    x    y    z  x_shear  y_shear\n",
       "0           0 -0.000228  0.000324 -0.000047  0.0  0.0  0.0      NaN      NaN\n",
       "1           1 -0.000644 -0.000226  0.001464  0.0  0.0  0.0      0.0      0.0\n",
       "2           2  0.019416  0.034281 -0.470873  0.0  0.0  1.7      0.0      0.0\n",
       "3           3  0.020019  0.034907 -0.463959  0.0  0.0  1.7      0.0      0.2\n",
       "4           4  0.019774  0.035586 -0.460220  0.0  0.0  1.7      0.0      0.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "242970e5-8dd0-4bae-bbff-6b8e0660990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying image transforms\n",
    "image_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        transforms.ConvertImageDtype(dtype=torch.float32)\n",
    "        #transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406],\n",
    "        #                     [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"valid\": transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406],\n",
    "        #                     [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    \"test\": transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406],\n",
    "        #                     [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4855aef-a91c-4bf1-b25a-43c455bfecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "square_labels_file = square_labels_dir + \"/square_data.csv\"\n",
    "sphere_labels_file = sphere_labels_dir + \"/sphere_data.csv\"\n",
    "romboid_labels_file = romboid_labels_dir + \"/romboid_data.csv\"\n",
    "\n",
    "# Loading individual datasets\n",
    "square_dataset = SquareImageDataset(annotations_file=square_labels_file, img_dir=square_imgs_dir, transform=image_transforms[\"train\"])#, target_transform=transforms.ToTensor())\n",
    "sphere_dataset = SphereImageDataset(annotations_file=sphere_labels_file, img_dir=sphere_imgs_dir, transform=image_transforms[\"train\"])#, target_transform=transforms.ToTensor())\n",
    "romboid_dataset = RomboidImageDataset(annotations_file=romboid_labels_file, img_dir=romboid_imgs_dir, transform=image_transforms[\"train\"])#, target_transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "\n",
    "# Dataset containing square and sphere imgs including labels\n",
    "full_dataset = ConcatDataset([square_dataset, romboid_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fd03a85-e7ac-4f01-8e29-427364749bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003988280889054832"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viz DEMO\n",
    "img, label = full_dataset[0]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d30b5558-56df-4741-b57c-718400f4c526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0745, 0.0745, 0.0706,  ..., 0.1098, 0.0980, 0.0745],\n",
       "         [0.0824, 0.0784, 0.0706,  ..., 0.0902, 0.1020, 0.0902],\n",
       "         [0.0824, 0.0784, 0.0745,  ..., 0.0745, 0.0980, 0.0863],\n",
       "         ...,\n",
       "         [0.0784, 0.0745, 0.0706,  ..., 0.0941, 0.0902, 0.0784],\n",
       "         [0.0784, 0.0706, 0.0667,  ..., 0.0863, 0.0902, 0.0824],\n",
       "         [0.0706, 0.0627, 0.0627,  ..., 0.0824, 0.0902, 0.0824]],\n",
       "\n",
       "        [[0.1373, 0.1529, 0.1569,  ..., 0.2863, 0.2784, 0.2471],\n",
       "         [0.1373, 0.1451, 0.1490,  ..., 0.2627, 0.2667, 0.2392],\n",
       "         [0.1255, 0.1333, 0.1451,  ..., 0.2510, 0.2431, 0.2039],\n",
       "         ...,\n",
       "         [0.1176, 0.1176, 0.1216,  ..., 0.1686, 0.1608, 0.1490],\n",
       "         [0.1216, 0.1176, 0.1176,  ..., 0.1529, 0.1490, 0.1412],\n",
       "         [0.1176, 0.1176, 0.1176,  ..., 0.1451, 0.1412, 0.1333]],\n",
       "\n",
       "        [[0.0667, 0.0784, 0.0824,  ..., 0.1373, 0.1333, 0.1255],\n",
       "         [0.0706, 0.0745, 0.0784,  ..., 0.1569, 0.1490, 0.1255],\n",
       "         [0.0588, 0.0667, 0.0824,  ..., 0.2235, 0.1765, 0.1216],\n",
       "         ...,\n",
       "         [0.0667, 0.0627, 0.0627,  ..., 0.1176, 0.1020, 0.0863],\n",
       "         [0.0667, 0.0627, 0.0588,  ..., 0.0941, 0.0863, 0.0745],\n",
       "         [0.0627, 0.0588, 0.0588,  ..., 0.0824, 0.0784, 0.0667]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aaa2e4c-a8cf-4f4d-84e6-0abcd00bd0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0745, 0.0745, 0.0706,  ..., 0.1098, 0.0980, 0.0745],\n",
       "         [0.0824, 0.0784, 0.0706,  ..., 0.0902, 0.1020, 0.0902],\n",
       "         [0.0824, 0.0784, 0.0745,  ..., 0.0745, 0.0980, 0.0863],\n",
       "         ...,\n",
       "         [0.0784, 0.0745, 0.0706,  ..., 0.0941, 0.0902, 0.0784],\n",
       "         [0.0784, 0.0706, 0.0667,  ..., 0.0863, 0.0902, 0.0824],\n",
       "         [0.0706, 0.0627, 0.0627,  ..., 0.0824, 0.0902, 0.0824]],\n",
       "\n",
       "        [[0.1373, 0.1529, 0.1569,  ..., 0.2863, 0.2784, 0.2471],\n",
       "         [0.1373, 0.1451, 0.1490,  ..., 0.2627, 0.2667, 0.2392],\n",
       "         [0.1255, 0.1333, 0.1451,  ..., 0.2510, 0.2431, 0.2039],\n",
       "         ...,\n",
       "         [0.1176, 0.1176, 0.1216,  ..., 0.1686, 0.1608, 0.1490],\n",
       "         [0.1216, 0.1176, 0.1176,  ..., 0.1529, 0.1490, 0.1412],\n",
       "         [0.1176, 0.1176, 0.1176,  ..., 0.1451, 0.1412, 0.1333]],\n",
       "\n",
       "        [[0.0667, 0.0784, 0.0824,  ..., 0.1373, 0.1333, 0.1255],\n",
       "         [0.0706, 0.0745, 0.0784,  ..., 0.1569, 0.1490, 0.1255],\n",
       "         [0.0588, 0.0667, 0.0824,  ..., 0.2235, 0.1765, 0.1216],\n",
       "         ...,\n",
       "         [0.0667, 0.0627, 0.0627,  ..., 0.1176, 0.1020, 0.0863],\n",
       "         [0.0667, 0.0627, 0.0588,  ..., 0.0941, 0.0863, 0.0745],\n",
       "         [0.0627, 0.0588, 0.0588,  ..., 0.0824, 0.0784, 0.0667]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67706164-8798-47b6-8a11-02c8b9352495",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = train_val_dataset(full_dataset)\n",
    "\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size = len(split_datasets[\"train\"])\n",
    "valid_data_size = len(split_datasets[\"valid\"])\n",
    "\n",
    "# bath size\n",
    "bs = 32\n",
    "\n",
    "train_data_loader = DataLoader(split_datasets[\"train\"], batch_size=bs, shuffle=True)\n",
    "valid_data_loader = DataLoader(split_datasets[\"valid\"], batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cfadbe5-892e-464e-a7a4-5670f1d952ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corcasta/miniconda3/envs/thesis/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/corcasta/miniconda3/envs/thesis/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/corcasta/miniconda3/envs/thesis/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Choose whatever GPU device number you want\n",
    "device = \"cuda\" #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "alexnet = models.alexnet(pretrained=True).to(device)\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "model = vgg16\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e7d57d2-cc68-4275-a83c-29bea73380cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose a model\n",
    "#model = vgg16\n",
    "#PATH = \"model.pt\"\n",
    "#torch.save(model.state_dict(), PATH)\n",
    "\n",
    "## Choose whatever GPU device number you want\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
    "## Make sure to call input = input.to(device) on any input tensors that you feed to the model\n",
    "#model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Each output represents a Force (fx, fy, fz)\n",
    "num_output = 3\n",
    "\n",
    "# A single output representing the magnitud force (fx, fy, fz)\n",
    "num_output = 1\n",
    "\n",
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "model.classifier[-1] = nn.Linear(4096, num_output)\n",
    "\n",
    "\n",
    "# Freeze model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze last layer parameters\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd865bb5-55ca-46ff-9807-e01a484686de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d460791-0981-4c48-b2d3-a080726f9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate the loss for each ourput neuron\n",
    "        # in this case 3 losses one for each force\n",
    "        individual_force_mean_square = ((inputs - targets)**2).mean(0)\n",
    "        loss = individual_force_mean_square.sum()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Calculate the loss for each ourput neuron\n",
    "        # in this case 3 losses one for each force\n",
    "        loss = ((inputs - targets)**2).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bdea0a5-7736-466f-8f3c-53e3999dc42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_func = nn.L1Loss()\n",
    "#loss_func = nn.MSELoss()\n",
    "loss_func = MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1d8e440-f663-46d7-aca6-a218e2b27712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "Epoch : 001, Training: Loss: 2568.7776, \n",
      "\t\tValidation : Loss : 2791.0606, Time: 44.9692s\n",
      "Epoch: 2/5\n",
      "Epoch : 002, Training: Loss: 2541.7895, \n",
      "\t\tValidation : Loss : 2818.1861, Time: 44.8988s\n",
      "Epoch: 3/5\n",
      "Epoch : 003, Training: Loss: 2546.0413, \n",
      "\t\tValidation : Loss : 3039.5365, Time: 44.8968s\n",
      "Epoch: 4/5\n",
      "Epoch : 004, Training: Loss: 2558.3024, \n",
      "\t\tValidation : Loss : 2813.5336, Time: 44.8613s\n",
      "Epoch: 5/5\n",
      "Epoch : 005, Training: Loss: 2506.7424, \n",
      "\t\tValidation : Loss : 2817.8927, Time: 45.1570s\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "trained_model, history = train_and_validate(model, loss_func, optimizer, num_epochs, train_data_loader, valid_data_loader, device)\n",
    "\n",
    "torch.save(trained_model.state_dict(), 'trained_model_e2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56efff00-f26e-4e7e-8abc-36f88c0a2318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c820deb-aaac-46ea-b292-0bbf4dee3d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd801de-a9b3-46e2-bbf5-32219b73602e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf393b04-935a-4d66-bdfe-8f6d01d54eeb",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e54e146-6399-445a-bf2f-6b5e5a87a777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"trained_model_e2.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90dcf8ae-fe84-465d-87ed-79f99dbf1032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 0.8102925533710709\n"
     ]
    }
   ],
   "source": [
    "img, label = split_datasets[\"valid\"][150]\n",
    "print(f\"Output: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0714bc59-577a-4d2d-aaef-cb6ca82f6ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ac13c2e-3db7-475b-8dfd-d27408f41430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = img[None, :, :, :]\n",
    "dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3b1739c-7c0e-4d20-bcd3-12bb93c8533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = model(dummy.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2b8406c2-a206-43c1-aca3-c651c05af969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.7374]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cde7f9-6d3f-4cf7-b54e-05135c2c2989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da5a29-73f8-4d87-bf16-f65068bec3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb20f39-6b0c-4c55-afce-0a67b2d39286",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,2,3,], [4,5,6]])\n",
    "b = torch.Tensor([[10,20,30,], [40,50,60]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13c023-58ef-4a15-a513-6a842cd0d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "(a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e8d09-7680-400d-863b-9d9e2f514128",
   "metadata": {},
   "outputs": [],
   "source": [
    "((a-b)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91737612-144f-4679-92b9-36e58eb5ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "((a-b)**2).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebfa33-3039-4fd9-b971-c2c91a36b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "((a-b)**2).mean(axis=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4500aee-badd-45f0-837a-71d1e662c8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
